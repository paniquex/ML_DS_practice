{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data_toxic/toxic_train.csv', index_col=0)\n",
    "test_data = pd.read_csv('data_toxic/toxic_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1, 'Распределение классов в трейне')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAFwCAYAAACYfpFkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAflUlEQVR4nO3de7RdZX3u8e9DAEFRAYmKBIVq2gpWUVNg6Glr1UKw9kA7vGBVImqjHmxraz3ipcUbx9qjtXK8FSUCrYrUK8eDYqRehwpERa6lRKQSQQgGFLxgA7/zx3x3XYa1k52Etfd+k+9njDX2XL/5zrneuefaz373u+ZeK1WFJKkfO8x1ByRJm8fglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtad5Jcvcky5PslOQxSR4z132aTwzuDiS5OslPk9ya5Pok70uy21z3S5qUqvoJ8FjgeuDdwA/mtkfzS/wHnPkvydXA86vqs0n2Ac4BPllVx89tzyTNBUfcnamq7wGfAh4GkOTYJJcnuSXJVUleMNo+yZFJLkzyoyTfTrK01T+f5GdtFH9rG9FfPbLd1UlekeSyJDe1Uf4uI+uf3PZ7c5KvJHn4Bo/7z0l+PrLvNSPr7pbkzUm+2/6CeHeSXUfW75ekRvp2e5Lnt3U7JDm+HcsPkpyZZM8Ntttxg368pi0/boN+PK21f/5I7bnt+3lTknOSPGjcedjwsZL8jySXJrnPSJtTN/I9eFuSa9p5+XqS3xpZtyDJK9sx3tLW79vWHZhkZZJ17Xv3ypHv6T8kubbd/iHJ3UaO+47Wj1uSnJ/kYdMc1+a0vbm1+1k7R1Pn65kj35/lrT/XJXnpyLYzPo9JDm733zDuPLbal5M8Z3PPY68M7s60H+AnAd9spRuAJwP3Ao4F3prkUa3twcDpwMuA3YHfBq4e2d2Lq2q3qtoN+IMxD/dM4HDgwcCvAq9u+30UsAJ4AXAf4B+Bs6aCYqqrwIlt30dssN83tf0dBDwE2Af4m5H1U8/Le7ftvzSy7s+Ao4DfAR4A3AS8Y0zfNyrJTsDrgetGakcBrwT+CFjYHveDM9jX0cBfAYdX1eif9DsAb5rme3ABw/HvCXwA+Jf84hfjXwLPYDjP9wKeC/wkyT2BzwKfZjj2hwDntm1eBRza9vkI4GDa+Wqubf3YHfgW8JqNHNKM2lbV7q3dC4GvTj2Xqur9I81+F1gMHAYcn+SJrb455/HvgO9tpL+/ZEvPY08M7n58PMnNwJeBLwD/C6Cq/l9VfbsGXwA+A0yN3p4HrKiqlVV1R1V9r6r+bTMe8+1VdU1VrQNOZAgTgD8B/rGqzquq26vqNOA2huCYsivw8w13mCRt+7+oqnVVdUs7lqNHmu0M3FFVt4/p0wuAV1XVmqq6jSFUnjI6yp6hFwDnAf++Qe2NVXV5Va1v/TpoE6O1pcApwBFVtWaDdTsz5nsAUFX/XFU/qKr1VfUW4G7Ar7XVzwdeXVVXtPP6rfYL4cnA96vqLVX1s6q6parOa9s8E3hdVd1QVWuB1wLPHvPQOwALmNmc8ea0nc5rq+rHVXUx8D5+8Rya0XlM8uTWj89uxmNuyXnsyuY+2TV3jqqqOz15kxwBnMAwgt0BuDtwcVu9L3D2VjzmNSPL/8EwMgJ4ELAsyZ+OrN95ZD3A/YG1Y/a5sPXx60OGA8PofMFImz0ZRmDjPAj4WJI7Rmq3A/cbuX/jyL7vTvsl918PNoxc/yfDL7jTNtj325K8ZbQ5w18E/zFNf97L8FfM7wBXbLBu2uNo0wbPZ/ieFcPIeq+2el/g22M2m65O289oH0fPF8AD2i/+XVqffm+a/Wxu203Z8Dn0G215JudxB+CNDL/oXzhNH6fsxnAupva9ueexK464O9amJj4CvBm4X1XtzhDUU6l1DcM0x5bad2T5gcC1I/s9sf2pPHW7e1V9sPVrJ4Y5+G+N2eeNwE+BA0e2nZoSmfKr/PJIeNQ1DKPb0cfepc39T9lrah1w5ph9vAw4s6o2/CG+BnjBBvvetaq+Mk1fYBhBPh04cWoeelPH0eazXw48Ddij9fOHbPq8bex8XssQWFNGzxcM0x+7M/wldDzD82Y6m9N2Uzb2HNrUeXwOcEVVfW26Po6c59E2W3Ieu2Jw921nhj+x1wLr2+j7sJH1pwDHJnlCezFonyS/vhn7Py7Jovai0SuBD7X6e4AXJjkkg3sk+f02koVhrv37wKoNd1hVd7Tt35rkvgCtX4e35X2BPwc+Pk2f3s0Qkg9q7RcmOXIzjumerX8nTrPvVyQ5sO373kmeuon9famqLgFOYpjrJ8mOSV7IMAr88jR9WM9w3nZM8jcMI+4p7wVen2Rx+/4+PMOLnp8E7p/kJe3FyHsmOaRt80Hg1e37sRfDawb/vOED13AZ2R38YnQ/rc1puxF/neGa7AMZvu9Tz6GZnMdXAa/YgsfckvPYFYO7Y21++M8YRpU3AX8MnDWy/nzaC5YMI7ov8Mujsk35AMOc+VXt9oa231UMf76+vT3uaobREUmeyRBg+wO3JLmV4SqYByR5d9vvy9s2X0vyI4b5y6n53XOAz7c+j/O2doyfSXILw0jrkGnajnMv4KSqutMURlV9jOGF0zNavy7hzi8qTueNwN5JljG8tnAscGS7HnlD5zB8T/6d4U/3n/HLUwp/z3BOPwP8iOEX8K7tfP8ewwvJ3weuZHjxD4Zzswq4iGGq7ButNuUB7YqPWxh+CT93I8eyOW035QsM5/pc4M1V9ZlWn8l5/GRVXbm5D7iV57ELXsetsTJy7fhmbvccYL+qes0G9UXAG6rqOXdRFzWPJdkP+A6wU3uBUHchR9y6q/2YYZS4ofXAulnui7RN8qoS3aWq6l+mqX+f4fpkSVvJqRJJ6oxTJZLUme1uqmTp0qX16U9/eq67IUkzkXHF7W7EfeONN851FyRpq2x3wS1JvTO4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktSZ7e5tXbfGo192+lx3YVZ9/X8fM9ddkDSGI25J6ozBLUmdMbglqTMTC+4kuyQ5P8m3klya5LWtfmqS7yS5sN0OavUkOSnJ6iQXJXnUyL6WJbmy3ZaN1B+d5OK2zUlJxn7MjyRtSyb54uRtwOOr6tYkOwFfTvKptu5lVfXhDdofASxut0OAdwGHJNkTOAFYAhTw9SRnVdVNrc1y4GvA2cBS4FNI0jZsYiPuGtza7u7UbrWRTY4ETm/bfQ3YPcnewOHAyqpa18J6JbC0rbtXVX21qgo4HThqUscjSfPFROe4kyxIciFwA0P4ntdWndimQ96a5G6ttg9wzcjma1ptY/U1Y+rj+rE8yaokq9auXbvVxyVJc2miwV1Vt1fVQcAi4OAkDwNeAfw68JvAnsDLW/Nx89O1BfVx/Ti5qpZU1ZKFCxdu5lFI0vwyK1eVVNXNwOeBpVV1XZsOuQ14H3Bwa7YG2Hdks0XAtZuoLxpTl6Rt2iSvKlmYZPe2vCvwRODf2tw07QqQo4BL2iZnAce0q0sOBX5YVdcB5wCHJdkjyR7AYcA5bd0tSQ5t+zoG+MSkjkeS5otJXlWyN3BakgUMvyDOrKpPJvnXJAsZpjouBF7Y2p8NPAlYDfwEOBagqtYleT1wQWv3uqpa15ZfBJwK7MpwNYlXlEja5k0suKvqIuCRY+qPn6Z9AcdNs24FsGJMfRXwsK3rqST1xf+clKTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMxML7iS7JDk/ybeSXJrkta2+f5LzklyZ5ENJdm71u7X7q9v6/Ub29YpWvyLJ4SP1pa22OsnxkzoWSZpPJjnivg14fFU9AjgIWJrkUOBNwFurajFwE/C81v55wE1V9RDgra0dSQ4AjgYOBJYC70yyIMkC4B3AEcABwDNaW0napk0suGtwa7u7U7sV8Hjgw61+GnBUWz6y3aetf0KStPoZVXVbVX0HWA0c3G6rq+qqqvo5cEZrK0nbtInOcbeR8YXADcBK4NvAzVW1vjVZA+zTlvcBrgFo638I3Ge0vsE209UlaZs20eCuqtur6iBgEcMI+aHjmrWvmWbd5tbvJMnyJKuSrFq7du2mOy5J89isXFVSVTcDnwcOBXZPsmNbtQi4ti2vAfYFaOvvDawbrW+wzXT1cY9/clUtqaolCxcuvCsOSZLmzCSvKlmYZPe2vCvwROBy4HPAU1qzZcAn2vJZ7T5t/b9WVbX60e2qk/2BxcD5wAXA4naVys4ML2CeNanjkaT5YsdNN9liewOntas/dgDOrKpPJrkMOCPJG4BvAqe09qcA/5RkNcNI+2iAqro0yZnAZcB64Liquh0gyYuBc4AFwIqqunSCxyNJ88LEgruqLgIeOaZ+FcN894b1nwFPnWZfJwInjqmfDZy91Z2VpI74n5OS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzkwsuJPsm+RzSS5PcmmSP2/11yT5XpIL2+1JI9u8IsnqJFckOXykvrTVVic5fqS+f5LzklyZ5ENJdp7U8UjSfDHJEfd64KVV9VDgUOC4JAe0dW+tqoPa7WyAtu5o4EBgKfDOJAuSLADeARwBHAA8Y2Q/b2r7WgzcBDxvgscjSfPCxIK7qq6rqm+05VuAy4F9NrLJkcAZVXVbVX0HWA0c3G6rq+qqqvo5cAZwZJIAjwc+3LY/DThqMkcjSfPHrMxxJ9kPeCRwXiu9OMlFSVYk2aPV9gGuGdlsTatNV78PcHNVrd+gPu7xlydZlWTV2rVr74IjkqS5M/HgTrIb8BHgJVX1I+BdwIOBg4DrgLdMNR2zeW1B/c7FqpOraklVLVm4cOFmHoEkzS87TnLnSXZiCO33V9VHAarq+pH17wE+2e6uAfYd2XwRcG1bHle/Edg9yY5t1D3aXpK2WZO8qiTAKcDlVfX3I/W9R5r9IXBJWz4LODrJ3ZLsDywGzgcuABa3K0h2ZngB86yqKuBzwFPa9suAT0zqeCRpvpjkiPuxwLOBi5Nc2GqvZLgq5CCGaY2rgRcAVNWlSc4ELmO4IuW4qrodIMmLgXOABcCKqrq07e/lwBlJ3gB8k+EXhSRt0yYW3FX1ZcbPQ5+9kW1OBE4cUz973HZVdRXDVSeStN3wPyclqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnZlYcCfZN8nnklye5NIkf97qeyZZmeTK9nWPVk+Sk5KsTnJRkkeN7GtZa39lkmUj9Ucnubhtc1KSTOp4JGm+mOSIez3w0qp6KHAocFySA4DjgXOrajFwbrsPcASwuN2WA++CIeiBE4BDgIOBE6bCvrVZPrLd0gkejyTNCzMK7iTnzqQ2qqquq6pvtOVbgMuBfYAjgdNas9OAo9rykcDpNfgasHuSvYHDgZVVta6qbgJWAkvbuntV1VerqoDTR/YlSdusHTe2MskuwN2Bvdood2oq4l7AA2b6IEn2Ax4JnAfcr6qugyHck9y3NdsHuGZkszWttrH6mjH1cY+/nGFkzgMf+MCZdluS5qWNBjfwAuAlDCH9dX4R3D8C3jGTB0iyG/AR4CVV9aONTEOPW1FbUL9zsepk4GSAJUuWjG0jSb3Y6FRJVb2tqvYH/qqqfqWq9m+3R1TV2ze18yQ7MYT2+6vqo618fZvmoH29odXXAPuObL4IuHYT9UVj6pK0TZvRHHdV/Z8kj0nyx0mOmbptbJt2hccpwOVV9fcjq84Cpq4MWQZ8YqR+TLu65FDgh21K5RzgsCR7tOmaw4Bz2rpbkhzaHuuYkX1J0jZrU1MlACT5J+DBwIXA7a089YLgdB4LPBu4OMmFrfZK4G+BM5M8D/gu8NS27mzgScBq4CfAsQBVtS7J64ELWrvXVdW6tvwi4FRgV+BT7SZJ27QZBTewBDigXb0xI1X1ZcbPQwM8YUz7Ao6bZl8rgBVj6quAh820T5K0LZjpddyXAPefZEckSTMz0xH3XsBlSc4HbpsqVtV/n0ivJEnTmmlwv2aSnZAkzdyMgruqvjDpjkiSZmamV5Xcwi/+uWVnYCfgx1V1r0l1TJI03kxH3PccvZ/kKIY3fJIkzbItenfAqvo48Pi7uC+SpBmY6VTJH43c3YHhum7f80OS5sBMryr5g5Hl9cDVDG/DKkmaZTOd4z520h2RJM3MTD9IYVGSjyW5Icn1ST6SZNGmt5Qk3dVm+uLk+xjeve8BDB9W8H9bTZI0y2Ya3Aur6n1Vtb7dTgUWTrBfkqRpzDS4b0zyrCQL2u1ZwA8m2TFJ0ngzDe7nAk8Dvg9cBzyF9n7ZkqTZNdPLAV8PLGufsk6SPYE3MwS6JGkWzXTE/fCp0IbhU2kYPrVdkjTLZhrcO7TPewT+a8Q909G6JOkuNNPwfQvwlSQfZvhX96cBJ06sV5Kkac30PydPT7KK4Y2lAvxRVV020Z5Jksaa8XRHC2rDWpLm2Ba9raskae4Y3JLUGYNbkjpjcEtSZ7wWW9Ks+u7rfmOuuzCrHvg3F9/l+3TELUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjozseBOsqJ9KvwlI7XXJPlekgvb7Ukj616RZHWSK5IcPlJf2mqrkxw/Ut8/yXlJrkzyoSQ7T+pYJGk+meSI+1Rg6Zj6W6vqoHY7GyDJAcDRwIFtm3dOfb4l8A7gCOAA4BmtLcCb2r4WAzcBz5vgsUjSvDGx4K6qLwLrZtj8SOCMqrqtqr4DrAYObrfVVXVVVf0cOAM4MkkY3mL2w23704Cj7tIDkKR5ai7muF+c5KI2lTL1qTr7ANeMtFnTatPV7wPcXFXrN6iPlWR5klVJVq1du/auOg5JmhOzHdzvAh4MHMTwafFvafWMaVtbUB+rqk6uqiVVtWThwoWb12NJmmdm9b1Kqur6qeUk7wE+2e6uAfYdaboIuLYtj6vfCOyeZMc26h5tL0nbtFkdcSfZe+TuHwJTV5ycBRyd5G5J9gcWA+cDFwCL2xUkOzO8gHlWVRXwOeApbftlwCdm4xgkaa5NbMSd5IPA44C9kqwBTgAel+QghmmNq4EXAFTVpUnOZPhotPXAcVV1e9vPi4FzgAXAiqq6tD3Ey4EzkrwB+CZwyqSORZLmk4kFd1U9Y0x52nCtqhMZ88nx7ZLBs8fUr2K46kSStiv+56QkdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ2ZWHAnWZHkhiSXjNT2TLIyyZXt6x6tniQnJVmd5KIkjxrZZllrf2WSZSP1Rye5uG1zUpJM6lgkaT6Z5Ij7VGDpBrXjgXOrajFwbrsPcASwuN2WA++CIeiBE4BDgIOBE6bCvrVZPrLdho8lSdukiQV3VX0RWLdB+UjgtLZ8GnDUSP30GnwN2D3J3sDhwMqqWldVNwErgaVt3b2q6qtVVcDpI/uSpG3abM9x36+qrgNoX+/b6vsA14y0W9NqG6uvGVMfK8nyJKuSrFq7du1WH4QkzaX58uLkuPnp2oL6WFV1clUtqaolCxcu3MIuStL8MNvBfX2b5qB9vaHV1wD7jrRbBFy7ifqiMXVJ2ubNdnCfBUxdGbIM+MRI/Zh2dcmhwA/bVMo5wGFJ9mgvSh4GnNPW3ZLk0HY1yTEj+5KkbdqOk9pxkg8CjwP2SrKG4eqQvwXOTPI84LvAU1vzs4EnAauBnwDHAlTVuiSvBy5o7V5XVVMveL6I4cqVXYFPtZskbfMmFtxV9YxpVj1hTNsCjptmPyuAFWPqq4CHbU0fJalH8+XFSUnSDBncktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1Zk6CO8nVSS5OcmGSVa22Z5KVSa5sX/do9SQ5KcnqJBcledTIfpa19lcmWTYXxyJJs20uR9y/W1UHVdWSdv944NyqWgyc2+4DHAEsbrflwLtgCHrgBOAQ4GDghKmwl6Rt2XyaKjkSOK0tnwYcNVI/vQZfA3ZPsjdwOLCyqtZV1U3ASmDpbHdakmbbXAV3AZ9J8vUky1vtflV1HUD7et9W3we4ZmTbNa02Xf1OkixPsirJqrVr196FhyFJs2/HOXrcx1bVtUnuC6xM8m8baZsxtdpI/c7FqpOBkwGWLFkyto0k9WJORtxVdW37egPwMYY56uvbFAjt6w2t+Rpg35HNFwHXbqQuSdu0WQ/uJPdIcs+pZeAw4BLgLGDqypBlwCfa8lnAMe3qkkOBH7aplHOAw5Ls0V6UPKzVJGmbNhdTJfcDPpZk6vE/UFWfTnIBcGaS5wHfBZ7a2p8NPAlYDfwEOBagqtYleT1wQWv3uqpaN3uHIUlzY9aDu6quAh4xpv4D4Alj6gUcN82+VgAr7uo+StJ8Np8uB5QkzYDBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZ7oP7iRLk1yRZHWS4+e6P5I0aV0Hd5IFwDuAI4ADgGckOWBueyVJk9V1cAMHA6ur6qqq+jlwBnDkHPdJkiZqx7nuwFbaB7hm5P4a4JANGyVZDixvd29NcsUs9K17efOyvYAb57of2uZsX8+rE7I1W3+6qpZuWOw9uMd9R+pOhaqTgZMn351tS5JVVbVkrvuhbYvPq63X+1TJGmDfkfuLgGvnqC+SNCt6D+4LgMVJ9k+yM3A0cNYc90mSJqrrqZKqWp/kxcA5wAJgRVVdOsfd2pY4vaRJ8Hm1lVJ1pylhSdI81vtUiSRtdwxuSepM13Pc2jxJbgcuHikdVVVXT9N2P+CTVfWwyfdMPUtyH+Dcdvf+wO3A2nb/4PbPcboLGdzbl59W1UFz3QltW6rqB8BBAEleA9xaVW8ebZMkDK+p3TH7Pdz2OFWynUuyX5IvJflGuz1mTJsDk5yf5MIkFyVZ3OrPGqn/Y3vvGAmAJA9JckmSdwPfAPZNcvPI+qOTvLct3y/JR5Osas+pQ+eq3z0wuLcvu7aQvTDJx1rtBuD3qupRwNOBk8Zs90LgbW20vgRYk+Shrf1jW/124JmTPwR15gDglKp6JPC9jbQ7Cfi79h+VTwPeOxud65VTJduXcVMlOwFvTzIVvr86ZruvAq9Ksgj4aFVdmeQJwKOBC4a/gtmV4ZeANOrbVXXBDNo9Efi19lwC2CPJrlX108l1rV8Gt/4CuB54BMNfYD/bsEFVfSDJecDvA+ckeT7D+8ScVlWvmM3Oqjs/Hlm+g19+f6FdRpaDL2TOmFMlujdwXXvR6NkM/4H6S5L8CnBVVZ3E8JYCD2e4iuApSe7b2uyZ5EGz1231pj3HbkqyOMkOwB+OrP4scNzUnfYXoKZhcOudwLIkX2OYJvnxmDZPBy5JciHw68DpVXUZ8GrgM0kuAlYCe89Sn9WvlwOfZvjFv2akfhzw2Pbi92XAn8xF53rhv7xLUmcccUtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNzariX5yhZs88qteLwHJPnwlm4vgddxS5stya1Vtdtc90PbL0fc2q4lubV93TvJF9s7J16S5Lemaf+3/OJdFt/fan/ZtrkkyUta7TfbfwHukuQeSS5N8rD2NrqXtDYLkrw5ycWt7Z/O0mGrc464tV2bGj0neSmwS1Wd2N5X/O5VdcvGtmnLjwZOBQ5leKOk84BnVdU3k7yB4Y2UdgXWVNUbRz9ZKMmLGN4V7+lVtT7JnlW1brJHrG2B7w4oDS4AViTZCfh4VV04w+3+G/CxqvoxQJKPAr8FfBN4Xdvvz4A/G7PtE4F3V9V6AENbM+VUiQRU1ReB32Z4s/9/SnLMDDfNRtbtCewG3JNffgvT0W39k1ebzeCWgPaWtDdU1XuAU4BHbaT5f7aROcAXgaOS3D3JPRjeqvRLbd3JwF8D7wfeNGY/nwFemGTH1oc9t/5ItD1wqkQaPA54WZL/BG4FNjbiPhm4KMk3quqZSU4Fzm/r3tvmt48B1rcPoVgAfCXJ44GrRvbzXoa30r2oPe57gLffpUelbZIvTkpSZ5wqkaTOOFUiTaN9zubdNig/u6ounov+SFOcKpGkzjhVIkmdMbglqTMGtyR1xuCWpM78f7zxz0YYkGYVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot(x='is_toxic', data=train_data, hue='is_toxic', kind='count')\n",
    "plt.title('Распределение классов в трейне')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = train_data['is_toxic'].map({True : 1, False : -1})\n",
    "train_data.drop(columns='is_toxic', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Приводим к нижнему регистру и заменяем все символы, не являющиеся буквами или цифрами на пробелы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_processed_1 = train_data.copy()\n",
    "test_data_processed_1 = test_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_process_1(texts_series):\n",
    "    \"\"\"\n",
    "    Takes series with texts\n",
    "    returns series with preprocessed texts\n",
    "    (lower register + changed all symbols, which are not letter, number to spaces)\n",
    "    \"\"\"\n",
    "    \n",
    "    texts_series = texts_series.str.lower()\n",
    "    texts_series = texts_series.apply(lambda x: re.sub(r'\\W', ' ', x))\n",
    "    return texts_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_processed_1 = text_process_1(train_data_processed_1['comment_text'])\n",
    "test_data_processed_1 = text_process_1(test_data_processed_1['comment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Пользуемся CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countVect = CountVectorizer(min_df=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_processed_2 = countVect.fit_transform(train_data_processed_1)\n",
    "test_data_processed_2 = countVect.transform(test_data_processed_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37832"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_processed_2.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optimization\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_feature_train = np.ones(train_data.shape[0])\n",
    "bias_feature_test = np.ones(test_data.shape[0])\n",
    "bias_feature_train = scipy.sparse.csr_matrix(bias_feature_train).T\n",
    "bias_feature_test = scipy.sparse.csr_matrix(bias_feature_test).T\n",
    "train_data_processed_2_with_bias = scipy.sparse.hstack([bias_feature_train, train_data_processed_2])\n",
    "test_data_processed_2_with_bias = scipy.sparse.hstack([bias_feature_test, test_data_processed_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 µs, sys: 0 ns, total: 24 µs\n",
      "Wall time: 3.81 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def experiment_cycle(train_data, target,\n",
    "                     step_alpha_list, step_beta_list,\n",
    "                     w_0_list, batch_size_list, l2_coef=5e-4):\n",
    "    for step_alpha in step_alpha_list:\n",
    "        for step_beta in step_beta_list:\n",
    "            for w_0 in w_0_list:\n",
    "#                 print(step_alpha)\n",
    "                clf = optimization.GDClassifier(step_alpha=step_alpha,\n",
    "                                                step_beta=step_beta,\n",
    "                                                experiment=True,\n",
    "                                                l2_coef=l2_coef,\n",
    "                                                max_iter=5000)\n",
    "                yield clf.fit(train_data,\n",
    "                                target,\n",
    "                                w_0=w_0,\n",
    "                                trace=True)\n",
    "                \n",
    "    for step_alpha in step_alpha_list:\n",
    "        for step_beta in step_beta_list:\n",
    "            for w_0 in w_0_list:\n",
    "                for batch_size in batch_size_list:\n",
    "                    clf = optimization.SGDClassifier(step_alpha=step_alpha,\n",
    "                                                     step_beta=step_beta,\n",
    "                                                     experiment=True,\n",
    "                                                     max_iter=5000,\n",
    "                                                     batch_size=batch_size, \n",
    "                                                     l2_coef=l2_coef)\n",
    "                    yield clf.fit(train_data,\n",
    "                                  target,\n",
    "                                  w_0=w_0,\n",
    "                                  trace=True, log_freq=0.1)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52061, 37832)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_processed_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ВАЖНО! Все эксперименты будут проводиться с добавлением смещения в веса, которое не будет учитываться в регуляризации (чтобы даже в том случае, когда регуляризация занулит почти все веса, наша разделяющая гиперплоскость не вырождалась в 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a527073318413aa709ed4daaf17086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tqdm\n",
    "history_list = []\n",
    "step_alpha_list = [0.001, 0.01, 0.1, 1, 5, 10, 15, 20, 25]\n",
    "# step_alpha_list = [0.1]\n",
    "step_beta_list = [0, 0.001, 0.01, 0.1, 1, 5, 10, 15, 20, 25]\n",
    "batch_size_list = [1, 128, 256, 512, 1024, 1024 * 4, 1024 * 10, 1024 * 20]\n",
    "w_0_list = [None]\n",
    "for history in tqdm.tqdm_notebook(experiment_cycle(train_data_processed_2_with_bias, target_train.values,\n",
    "                                step_alpha_list,\n",
    "                                step_beta_list,\n",
    "                                w_0_list=w_0_list,\n",
    "                                batch_size_list=batch_size_list)):\n",
    "    history_list.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_for_df = ['time', 'func', 'accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(history_list[0]['time']), len(history_list[0]['func']), len(history_list[0]['accuracy']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_list[50]['classifier_type'], history_list[30]['step_alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_0_df = pd.DataFrame({key: history_list[29][key] for key in keys_for_df})\n",
    "hist_1_df = pd.DataFrame({key: history_list[300][key] for key in keys_for_df})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x = np.cumsum(np.arange(len(hist_0_df['accuracy']))), y=hist_0_df['accuracy'])\n",
    "sns.lineplot(x = np.cumsum(np.arange(len(hist_1_df['accuracy']))), y=hist_1_df['accuracy'])\n",
    "# plt.semilogy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('GD_SGD_step_alpha_beta_batch_size.pkl', 'wb') as f:\n",
    "    pickle.dump(history_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперименты с различными начальными приближениями:\n",
    "https://datascience.stackexchange.com/questions/30989/what-are-the-cases-where-it-is-fine-to-initialize-all-weights-to-zero\n",
    "###   Рассмотрим 7 экспериментов: \n",
    "    1) w_0 = 0\n",
    "    2) w_0 = uniform(0, 1)\n",
    "    3) w_0 = uniform(100, 500)\n",
    "    4) w_0 = uniform(1000, 5000)\n",
    "    5) w_0 = uniform (10000, 50000)\n",
    "    6) w_0 = normal with mean = 0, std = 1\n",
    "    7) w_0 = normal with mean = 0.5, std = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0_list = []\n",
    "w_0_list.append(np.zeros(train_data_processed_2_with_bias.shape[1]))\n",
    "w_0_list.append(np.random.uniform(0, 1, size=train_data_processed_2_with_bias.shape[1]))\n",
    "\n",
    "w_0_list.append(np.random.uniform(100, 500, size=train_data_processed_2_with_bias.shape[1]))\n",
    "w_0_list.append(np.random.uniform(1000, 5000, size=train_data_processed_2_with_bias.shape[1]))\n",
    "w_0_list.append(np.random.uniform(10000, 50000, size=train_data_processed_2_with_bias.shape[1]))\n",
    "w_0_list.append(np.random.normal(0, 1, size=train_data_processed_2_with_bias.shape[1]))\n",
    "w_0_list.append(np.random.normal(0.5, 0.5, size=train_data_processed_2_with_bias.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_processed_2_with_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0_list[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Выберем лучшие параметры из прошлого эксперимента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "history_list = []\n",
    "with open('GD_SGD_step_alpha_beta_batch_size.pkl', 'rb') as f:\n",
    "    history_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_acc = [0, 0]\n",
    "arg_max = [0, 0]\n",
    "for i, history in enumerate(history_list):\n",
    "    if history['classifier_type'] == 'GD':\n",
    "        if np.max(history['accuracy']) > max_acc[0]:\n",
    "            max_acc[0] = np.max(history['accuracy'])\n",
    "            arg_max[0] = i\n",
    "    else:\n",
    "        if np.max(history['accuracy']) > max_acc[1]:\n",
    "            max_acc[1] = np.max(history['accuracy'])\n",
    "            arg_max[1] = i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history_list[arg_max[0]]['step_alpha'],\n",
    "      history_list[arg_max[0]]['step_beta'],\n",
    "      history_list[arg_max[0]]['classifier_type'],\n",
    "      max_acc[1])\n",
    "\n",
    "print(history_list[arg_max[1]]['step_alpha'],\n",
    "      history_list[arg_max[1]]['step_beta'],\n",
    "      history_list[arg_max[1]]['classifier_type'],\n",
    "      max_acc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "step_alpha_list = [history_list[arg_max[0]]['step_alpha']]\n",
    "step_beta_list = [history_list[arg_max[0]]['step_beta']]\n",
    "batch_size_list = [history_list[arg_max[1]]['batch_size']]\n",
    "history_list = []\n",
    "for history in tqdm.tqdm_notebook(experiment_cycle(train_data_processed_2_with_bias, target_train.values,\n",
    "                                step_alpha_list,\n",
    "                                step_beta_list,\n",
    "                                w_0_list=w_0_list,\n",
    "                                batch_size_list=batch_size_list)):\n",
    "    history_list.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(history_list[0]['accuracy']), max(history_list[-1]['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('GD_SGD_w_0_init.pkl', 'wb') as f:\n",
    "    pickle.dump(history_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('GD_SGD_w_0_init.pkl', 'rb') as f:\n",
    "    history_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_acc = [0, 0]\n",
    "arg_max = [0, 0]\n",
    "for i, history in enumerate(history_list):\n",
    "    if history['classifier_type'] == 'GD':\n",
    "        if np.max(history['accuracy']) > max_acc[0]:\n",
    "            max_acc[0] = np.max(history['accuracy'])\n",
    "            arg_max[0] = i\n",
    "    else:\n",
    "        if np.max(history['accuracy']) > max_acc[1]:\n",
    "            max_acc[1] = np.max(history['accuracy'])\n",
    "            arg_max[1] = i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history_list[arg_max[0]]['step_alpha'],\n",
    "      history_list[arg_max[0]]['step_beta'],\n",
    "      history_list[arg_max[0]]['classifier_type'],\n",
    "      max_acc[0],\n",
    "      arg_max[0] + 1)\n",
    "\n",
    "print(history_list[arg_max[1]]['step_alpha'],\n",
    "      history_list[arg_max[1]]['step_beta'],\n",
    "      history_list[arg_max[1]]['classifier_type'],\n",
    "      max_acc[1],\n",
    "      arg_max[1] - 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_processed_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_data_processed_lemmatized = train_data_processed_1.apply(lambda x: x.split()).apply(lambda x: [lemmatizer.lemmatize(elem) for elem in x])\n",
    "test_data_processed_lemmatized = test_data_processed_1.apply(lambda x: x.split()).apply(lambda x: [lemmatizer.lemmatize(elem) for elem in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_data_processed_3 = train_data_processed_lemmatized.apply(lambda x: [elem for elem in x if elem not in stopwords_set])\n",
    "train_data_processed_3 = train_data_processed_3.apply(lambda x: ' '.join(x))\n",
    "\n",
    "test_data_processed_3 = test_data_processed_lemmatized.apply(lambda x: [elem for elem in x if elem not in stopwords_set])\n",
    "test_data_processed_3 = test_data_processed_3.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_data_processed_3 = countVect.fit_transform(train_data_processed_3)\n",
    "test_data_processed_3 = countVect.transform(test_data_processed_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_feature_train = np.ones(train_data.shape[0])\n",
    "bias_feature_test = np.ones(test_data.shape[0])\n",
    "bias_feature_train = scipy.sparse.csr_matrix(bias_feature_train).T\n",
    "bias_feature_test = scipy.sparse.csr_matrix(bias_feature_test).T\n",
    "train_data_processed_3_with_bias = scipy.sparse.hstack([bias_feature_train, train_data_processed_3])\n",
    "test_data_processed_3_with_bias = scipy.sparse.hstack([bias_feature_test, test_data_processed_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Так как лучшими значениями для w_0 оказались:\n",
    "    1) GD - w_0 = zeros\n",
    "    2) SGD - w_0 = normal with mean = 0.5, std = 0.5, поэтому:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "step_alpha_list = [history_list[arg_max[0]]['step_alpha']]\n",
    "step_beta_list = [history_list[arg_max[0]]['step_beta']]\n",
    "batch_size_list = [history_list[arg_max[1]]['batch_size']]\n",
    "w_0_list = []\n",
    "w_0_list.append(np.zeros(train_data_processed_3_with_bias.shape[1]))\n",
    "w_0_list.append(np.random.normal(0.5, 0.5, size=train_data_processed_3_with_bias.shape[1]))\n",
    "\n",
    "# w_0_list.append(np.random.uniform(100, 500, size=train_data_processed_2_with_bias.shape[1]))\n",
    "# w_0_list.append(np.random.uniform(1000, 5000, size=train_data_processed_2_with_bias.shape[1]))\n",
    "# w_0_list.append(np.random.uniform(10000, 50000, size=train_data_processed_2_with_bias.shape[1]))\n",
    "# w_0_list.append(np.random.normal(0, 1, size=train_data_processed_2_with_bias.shape[1]))\n",
    "# w_0_list.append(np.random.normal(0.5, 0.5, size=train_data_processed_2_with_bias.shape[1]))\n",
    "history_list = []\n",
    "for history in tqdm.tqdm_notebook(experiment_cycle(train_data_processed_3_with_bias, target_train.values,\n",
    "                                step_alpha_list,\n",
    "                                step_beta_list,\n",
    "                                w_0_list=w_0_list,\n",
    "                                batch_size_list=batch_size_list)):\n",
    "    history_list.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_acc = [0, 0]\n",
    "arg_max = [0, 0]\n",
    "for i, history in enumerate(history_list):\n",
    "    if history['classifier_type'] == 'GD':\n",
    "        if np.max(history['accuracy']) > max_acc[0]:\n",
    "            max_acc[0] = np.max(history['accuracy'])\n",
    "            arg_max[0] = i\n",
    "    else:\n",
    "        if np.max(history['accuracy']) > max_acc[1]:\n",
    "            max_acc[1] = np.max(history['accuracy'])\n",
    "            arg_max[1] = i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history_list[arg_max[0]]['step_alpha'],\n",
    "      history_list[arg_max[0]]['step_beta'],\n",
    "      history_list[arg_max[0]]['classifier_type'],\n",
    "      max_acc[0],\n",
    "      arg_max[0] + 1)\n",
    "\n",
    "print(history_list[arg_max[1]]['step_alpha'],\n",
    "      history_list[arg_max[1]]['step_beta'],\n",
    "      history_list[arg_max[1]]['classifier_type'],\n",
    "      max_acc[1],\n",
    "      arg_max[1] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('GD_SGD_lemmatized.pkl', 'wb') as f:\n",
    "    pickle.dump(history_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('GD_SGD_lemmatized.pkl', 'rb') as f:\n",
    "    history_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_acc = [0, 0]\n",
    "arg_max = [0, 0]\n",
    "for i, history in enumerate(history_list):\n",
    "    if history['classifier_type'] == 'GD':\n",
    "        if np.max(history['accuracy']) > max_acc[0]:\n",
    "            max_acc[0] = np.max(history['accuracy'])\n",
    "            arg_max[0] = i\n",
    "    else:\n",
    "        if np.max(history['accuracy']) > max_acc[1]:\n",
    "            max_acc[1] = np.max(history['accuracy'])\n",
    "            arg_max[1] = i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history_list[arg_max[0]]['step_alpha'],\n",
    "      history_list[arg_max[0]]['step_beta'],\n",
    "      history_list[arg_max[0]]['classifier_type'],\n",
    "      max_acc[0],\n",
    "      arg_max[0] + 1)\n",
    "\n",
    "print(history_list[arg_max[1]]['step_alpha'],\n",
    "      history_list[arg_max[1]]['step_beta'],\n",
    "      history_list[arg_max[1]]['classifier_type'],\n",
    "      max_acc[1],\n",
    "      arg_max[1] - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперименты с min_df и max_df\n",
    "Если мы рассматриваем 0 <= min_df <= 1; 0 <= max_df <= 1, то они связаны формулой: $min\\_df + max\\_df = 1$, поэтому будем исследовать значение параметра $min\\_df$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "min_df_list = [1e-5, 2.5e-5, 5e-5, 7.5e-5,\n",
    "               1e-4, 2.5e-4, 5e-4, 7.5e-4,\n",
    "               1e-3, 2.5e-3, 5e-3, 7.5e-3]\n",
    "preprocesser_type_list = ['BOW', 'TFIDF']\n",
    "\n",
    "preprocesser = None\n",
    "\n",
    "step_alpha_list = [history_list[arg_max[0]]['step_alpha']]\n",
    "step_beta_list = [history_list[arg_max[0]]['step_beta']]\n",
    "batch_size_list = [history_list[arg_max[1]]['batch_size']]\n",
    "history_list = []\n",
    "\n",
    "\n",
    "for min_df in min_df_list:\n",
    "    for preprocesser_type in preprocesser_type_list:\n",
    "        train_data_processed_3 = train_data_processed_lemmatized.apply(lambda x: [elem for elem in x if elem not in stopwords_set])\n",
    "        train_data_processed_3 = train_data_processed_3.apply(lambda x: ' '.join(x))\n",
    "\n",
    "    #     test_data_processed_3 = test_data_processed_lemmatized.apply(lambda x: [elem for elem in x if elem not in stopwords_set])\n",
    "    #     test_data_processed_3 = test_data_processed_3.apply(lambda x: ' '.join(x))\n",
    "\n",
    "        if preprocesser_type == 'BOW':\n",
    "            preprocesser = CountVectorizer(min_df=min_df, lowercase=False)\n",
    "        else:\n",
    "            preprocesser = TfidfVectorizer(min_df=min_df, lowercase=False)\n",
    "\n",
    "        train_data_processed_3 = preprocesser.fit_transform(train_data_processed_3)\n",
    "    #     test_data_processed_3 = preprocesser.transform(test_data_processed_3)\n",
    "\n",
    "        bias_feature_train = np.ones(train_data.shape[0])\n",
    "    #     bias_feature_test = np.ones(test_data.shape[0])\n",
    "        bias_feature_train = scipy.sparse.csr_matrix(bias_feature_train).T\n",
    "    #     bias_feature_test = scipy.sparse.csr_matrix(bias_feature_test).T\n",
    "        train_data_processed_3_with_bias = scipy.sparse.hstack([bias_feature_train, train_data_processed_3])\n",
    "    #     test_data_processed_3_with_bias = scipy.sparse.hstack([bias_feature_test, test_data_processed_3])\n",
    "        w_0_list = []\n",
    "        w_0_list.append(np.zeros(train_data_processed_3_with_bias.shape[1]))\n",
    "        w_0_list.append(np.random.normal(0.5, 0.5, size=train_data_processed_3_with_bias.shape[1]))\n",
    "\n",
    "        for history in tqdm.tqdm_notebook(experiment_cycle(train_data_processed_3_with_bias, target_train.values,\n",
    "                                    step_alpha_list,\n",
    "                                    step_beta_list,\n",
    "                                    w_0_list=w_0_list,\n",
    "                                    batch_size_list=batch_size_list)):\n",
    "            \n",
    "            history['preprocesser_type'] = preprocesser_type\n",
    "            history['min_df'] = min_df\n",
    "            history_list.append(history)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('GD_SGD_bow_tfid_min_df.pkl', 'wb') as f:\n",
    "    pickle.dump(history_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('GD_SGD_bow_tfid_min_df.pkl', 'rb') as f:\n",
    "    history_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_acc = [0, 0]\n",
    "arg_max = [0, 0]\n",
    "for i, history in enumerate(history_list):\n",
    "    if history['classifier_type'] == 'GD':\n",
    "        if np.max(history['accuracy']) > max_acc[0]:\n",
    "            max_acc[0] = np.max(history['accuracy'])\n",
    "            arg_max[0] = i\n",
    "    else:\n",
    "        if np.max(history['accuracy']) > max_acc[1]:\n",
    "            max_acc[1] = np.max(history['accuracy'])\n",
    "            arg_max[1] = i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history_list[arg_max[0]]['step_alpha'],\n",
    "      history_list[arg_max[0]]['step_beta'],\n",
    "      history_list[arg_max[0]]['classifier_type'],\n",
    "      history_list[arg_max[0]]['min_df'],\n",
    "      history_list[arg_max[0]]['preprocesser_type'],\n",
    "      max_acc[0],\n",
    "      arg_max[0] + 1)\n",
    "\n",
    "print(history_list[arg_max[1]]['step_alpha'],\n",
    "      history_list[arg_max[1]]['step_beta'],\n",
    "      history_list[arg_max[1]]['classifier_type'],\n",
    "      history_list[arg_max[1]]['min_df'],\n",
    "      history_list[arg_max[1]]['preprocesser_type'],\n",
    "      max_acc[1],\n",
    "      arg_max[1] - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
